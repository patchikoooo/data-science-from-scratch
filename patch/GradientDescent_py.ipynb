{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientDescent.py",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patchikoooo/data-science-from-scratch/blob/master/patch/GradientDescent_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xue_3YcyUgah",
        "colab_type": "text"
      },
      "source": [
        "# **Gradient Descent**\n",
        "\n",
        "\"Those who boast about their descent, brag on what they owe to others.\"\n",
        "-- Seneca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQhsaE0xMEYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGcW-w6tx-R4",
        "colab_type": "code",
        "outputId": "9d20e49f-1795-4457-d996-5601150424ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuPRjgzw2fFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/data-science-from-scratch/libraries')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHhQxwxLxTgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from functools import partial\n",
        "from linear_algebra import Vector, dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUVnJtr4UdRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function that takes as input a vector of real numbers and outputs a single real number:\n",
        "def sum_of_squares(v):\n",
        "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
        "    #return sum(v_i ** 2 for v_i in v)\n",
        "    return dot(v, v)\n",
        "\n",
        "# We frequently need to maximize(or minimize) such functions. That is, we need to\n",
        "#       find the input v that produces the largest(or smallest) possible value.\n",
        "\n",
        "# One approach to maximizing a function is to pick a random starting point,\n",
        "#       compute the gradient, take a small step in the direction of the gradien\n",
        "#       (i.e, the direction that causes the cuntion to increase the most).\n",
        "#       and repeat with the new starting point. Similarly, you can try to\n",
        "#       minimize a function by taking small steps in the opposite direction\n",
        "\n",
        "### NOTE:\n",
        "# If a function has a unique global minimum, this procedure is likely to find it.\n",
        "#       If a function has multiple(local) minima, this procedure might \"find\" the\n",
        "#       wrong one of them, in which case you might re-run the procedure from\n",
        "#       a variety of starting points. If a function has no minimum, then it's\n",
        "#       possible the procedure might go on forever."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vtl6YvEvBML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" ESTIMATING THE GRADIENT\n",
        "If f is a function of one variable, its derivative at a point x measures how\n",
        "        f(x) changes when we make a very small change to x. It is defined as the\n",
        "        limit of the difference qoutients: \"\"\"\n",
        "from typing import Callable\n",
        "\n",
        "def difference_qoutient(f: Callable[[float], float],\n",
        "                        x: float,\n",
        "                        h: float) -> float:\n",
        "    # this is a function to get the derivative of the function\n",
        "    return ( f(x+h) - f(x) ) / h # as h approches 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3zIWtwtQOKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def square(x: float) -> float:\n",
        "    return x * x\n",
        "\n",
        "def derivative(x: float) -> float:\n",
        "    return 2 * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E63XXf3dRHkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from linear_algebra import distance, add, scalar_multiply"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzFdsVQRV3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
        "    \"\"\" Moves 'step_size in the 'gradient' direction from `v` \"\"\"\n",
        "    assert len(v) == len(gradient)\n",
        "    step = scalar_multiply(step_size, gradient)\n",
        "    return add(v, step)\n",
        "\n",
        "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
        "    return [2 * v_i for v_i in v]\n",
        "\n",
        "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
        "    slope, intercept = theta\n",
        "    predicted = slope * x + intercept       # The prediction of the model\n",
        "    error = (predicted - y)                 # error is (predicted - actual)\n",
        "    squared_error = error ** 2              # We'll minimize squared error\n",
        "    grad = [2 * error * x, 2 * error]       # using its gradient\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHqUNcdIS9wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import TypeVar, List, Iterator\n",
        "T = TypeVar('T')    # this allows us to type \"generic functions\"\n",
        "\n",
        "def minibatches(dataset: List[T],\n",
        "                batch_size: int,\n",
        "                shuffle: bool = True) -> Iterator[List[T]]:\n",
        "    \"\"\" Generated 'batch_size' -sized minibatches from the dataset \"\"\"\n",
        "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
        "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
        "\n",
        "    if shuffle: random.shuffle(batch_starts) # shuffle the batches\n",
        "\n",
        "    for start in batch_starts:\n",
        "        end = start + batch_size\n",
        "        yield dataset[start:end]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIQGE6HewUzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### NOTE:\n",
        "# We can't take limits in python\n",
        "# But we can estimate derivatives by evaluating the difference quotient for\n",
        "#       a very small e.\n",
        "\n",
        "#derivative_estimate = partial(difference_qoutient, square, h=0.00001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4XTTOPswovG",
        "colab_type": "code",
        "outputId": "1dd88bcf-4352-4a43-92e9-c7ea8f5ba495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "# plot to show they're basically the same\n",
        "from matplotlib import pyplot as plt\n",
        "x = range(-10, 10)\n",
        "actual = [derivative(i) for i in x]\n",
        "print(actual)\n",
        "estimates = [difference_qoutient(square, i, h=0.000001) for i in x]\n",
        "#derestx = [derivative_estimate(i) for i in x]\n",
        "print(estimates)\n",
        "\n",
        "plt.title(\"Actual Derivatives vs. Estimates\")\n",
        "plt.plot(x, actual, 'rx', label='Actual')               # red x\n",
        "plt.plot(x, estimates, 'b+', label='Estimate')   # blue +\n",
        "plt.legend(loc=9)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[-19.999998983166734, -17.999998988216248, -15.99999900037119, -13.999999005420705, -11.999999003364792, -9.999999001308879, -7.999999001029323, -5.999999000749767, -3.999998999582033, -1.999999000079633, 1e-06, 2.0000009999243673, 4.0000010006480125, 6.000001000927568, 8.000000999430767, 10.00000100148668, 12.00000099998988, 14.000001002045792, 16.000000982785423, 18.000000991946763]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEICAYAAAC6fYRZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZV7D8APivPca7ybNwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP6UKgLfW663",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# When f is a function of many varibales, it has multiple partial derivatives,\n",
        "#       each indicating how f changes when we make small changes in just one of\n",
        "#       input variables.\n",
        "\n",
        "# We calculate its ith partial derivative by treating it as a function of just\n",
        "#       its ith variable, holding the other variables fixed:\n",
        "\n",
        "def partial_difference_quotient(f: Callable[[Vector], float],\n",
        "                                v: Vector,\n",
        "                                i: int,\n",
        "                                h: float) -> float:\n",
        "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
        "    # add h to just the ith element of v\n",
        "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
        "\n",
        "    # we get the vector change in the specified axis i\n",
        "    return ( f(w) - f(v) ) / h\n",
        "    \n",
        "# after which, we can estimate the gradient the same way:\n",
        "def estimate_gradient(f: Callable[[Vector], float],\n",
        "                      v: Vector,\n",
        "                      h: float = 0.00001):\n",
        "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]\n",
        "\n",
        "### NOTE\n",
        "# A major drawback to this \"estimate using difference quotients\" approach is that\n",
        "#       it's computationally expensive. If v has length n, estimate_gradient\n",
        "#       has to evaluate f on 2n different inputs. If you're repeatedly estimating\n",
        "#       gradients, you're doing a whole lot of extra work"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Cu-HDuyXi2",
        "colab_type": "text"
      },
      "source": [
        "**Using the Gradient**\n",
        "\n",
        "It's easy to see htat the sum_of_squares is smallest when its input v is a vector of zeroes. But imagine we didn't know that. Let's use gradients to find the minimum among all three-dimentional vectors. We'll just pick a random starting point and then take tiny steps in the opposite direction of the graedient until we reach a point where the gradient is very small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6kq1ggHu_kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_step(v: Vector,\n",
        "                  gradient: Vector,\n",
        "                  step_size: float) -> Vector:\n",
        "    \"\"\"move step_size in the direction from v\"\"\"\n",
        "\n",
        "    \"\"\" Here, we are getting a new value for the weight of the error.\n",
        "            This might mean the m and b of the function error \"\"\"\n",
        "    \n",
        "    #return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]\n",
        "    assert len(v) == len(gradient)  # Make sure that vectors have the same size\n",
        "    step = scalar_multiply(step_size, gradient)\n",
        "    return add(v, step)\n",
        "\n",
        "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
        "    return [2 * v_i for v_i in v]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ahzhNTVpcN",
        "colab_type": "code",
        "outputId": "418f794a-a096-4d89-8f90-6a4ff4fbd2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\" EXAMPLE \"\"\"\n",
        "# Pick a random starting point\n",
        "v = [random.randint(-10, 10) for i in range(3)] # 3 dimensional vector\n",
        "tolerance = 0.0000001\n",
        "\n",
        "while True:\n",
        "    gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
        "    next_v = gradient_step(v, gradient, -0.01)       # take a negative gradient step\n",
        "    if distance(next_v, v) < tolerance:      # stop if we're converging\n",
        "        break\n",
        "    v = next_v\n",
        "print(v)\n",
        "##sum_of_squares_gradient(v)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.067398886847253e-07, 3.6269595547389013e-06, -3.1735896103965395e-06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBfn2Gl9dUTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "16c87cb1-8b35-403d-c594-501b8abc69b8"
      },
      "source": [
        "\"\"\" Choosing the Right Step Size\n",
        "Although the rationale for moving against the gradient is clear, how far\n",
        "        to move is not. Indeed, choosing the right step size is more of an art\n",
        "        than a science. Popular options include:\n",
        "\n",
        "    # Using a fixed step size\n",
        "    # Gradually shrinking the step size over time\n",
        "    # At each step, choosing the step size that minimizes the value of the\n",
        "            objective function\n",
        "    \n",
        "The last sounds optimal but is, in practive, a costly computation. We can\n",
        "        approximate it by trying a variety of step sizes and choosing the one\n",
        "        that results in the smallest value of the objective function: \"\"\"\n",
        "\n",
        "        #page 150\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Choosing the Right Step Size\\nAlthough the rationale for moving against the gradient is clear, how far\\n        to move is not. Indeed, choosing the right step size is more of an art\\n        than a science. Popular options include:\\n\\n    # Using a fixed step size\\n    # Gradually shrinking the step size over time\\n    # At each step, choosing the step size that minimizes the value of the\\n            objective function\\n    \\nThe last sounds optimal but is, in practive, a costly computation. We can\\n        approximate it by trying a variety of step sizes and choosing the one\\n        that results in the smallest value of the objective function: '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFavTUb5VvjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" It is possible that certain step sizes will result in invalid inputs for\n",
        "        our function. So we'll need to create a \"safe apply\" function that returns\n",
        "        infinity(which should never be the minimum of anything) for invalid\n",
        "        inputs. \"\"\"\n",
        "\n",
        "def safe(f):\n",
        "    \"\"\" return a new function that's the same as f, except that if outputs\n",
        "            infinity whenever f produces an error \"\"\"\n",
        "    def safe_f(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except:\n",
        "            return float('inf') # this means infinity in python\n",
        "    \n",
        "    return safe_f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guWoK_q-W5XG",
        "colab_type": "text"
      },
      "source": [
        "# **Putting it all Together**\n",
        "In the general case, we have some target_fn that we want to minimize, and we also have its gradient_fn.\n",
        "\n",
        "For example, the target_fn could represent the errors in a model as a function of its parameters, and we might want to find the parameters that make the errors as small as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB-izKz6WyzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Furthermore, let's say we have (somehow) chosen a starting value for the\n",
        "#       parameters theta_0. Then we can implement gradient descent as:\n",
        "\n",
        "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    # use gradient descent to find theta that minimizes the target function\n",
        "\n",
        "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.0001, 0.00001]\n",
        "\n",
        "    theta = theta_0                     # set theta to initial value\n",
        "    target_fn = safe(target_fn)         # safe version of target_fn\n",
        "    value = target_fn(theta)            # value we're minimizing\n",
        "\n",
        "    while True:\n",
        "        gradient = gradient_fn(theta)\n",
        "        next_thetas = [gradient_step(theta, gradient, -step_size)\n",
        "                        for step_size in step_sizes]\n",
        "\n",
        "        # choose the one that minimizes the error function\n",
        "        next_theta = min(next_thetas, key=target_fn)\n",
        "        next_value = target_fn(next_theta)\n",
        "\n",
        "        # stop if we're \"converging\"\n",
        "        if abs(value - next_value) < tolerance:\n",
        "            return theta\n",
        "        else:\n",
        "            theta, value = next_theta, next_value\n",
        "\n",
        "# We called it minimize_batch because, for each gradient step, it looks at the\n",
        "#       entire data set (because target_fn returns the error on the whole data set)\n",
        "#       In the next section, we'll see an alternative approach that only looks\n",
        "#       at one data point at a time.\n",
        "\n",
        "# Sometimes we'll instead want to maximize a function, which we can do by\n",
        "#       minimizing its negative(which has a corresponding negative gradient): "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtAXDUMvvQ3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def negate(f):\n",
        "     \"\"\" returns a function that for any input x returns -f(x)\"\"\"\n",
        "     return lambda *args, **kwags: -f(*args, **kwargs)\n",
        "\n",
        "def negate_all(f):\n",
        "    \"\"\" the same when f returns a list of numbers \"\"\"\n",
        "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
        "\n",
        "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    return minimize_batch(negate(target_fn),\n",
        "                          negate_all(gradient_fn).\n",
        "                          theta_0,\n",
        "                          tolerance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVNoGZ_0ggX",
        "colab_type": "text"
      },
      "source": [
        "**Stochastic Gradient Descent**\n",
        "As we mentioned before, often we'll be using gradient descent to choose the parameters of a model in a way that minimizes some notion of error. Using the precious batch approach, each gradient step requires us to make a prediction and compute the gradient for the whole data set, which makes each step take a long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vzkSo_30fpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Now, usually these error functions are additive, which means that the\n",
        "        predictive error on the whole data set is simply the sum of the\n",
        "        predictive errors for each data point.\n",
        "\n",
        "    When this is the case, we can instead apply a technique called stochastic\n",
        "        gradient descent, which computes the gradient (and takes a step) for only\n",
        "        one point at a time. It cycles over our data repeatedly until it reaches\n",
        "        a stopping point. \"\"\"\n",
        "\n",
        "# During each cycle, we'll want to iterate through our data in a random order:\n",
        "def in_random_order(data):\n",
        "    \"\"\" generator that returns the elements of data in random order\"\"\"\n",
        "    indexes = [i for i, _ in enumerate(data)]   # create a list of indexes\n",
        "    random.shuffle(indexes)\n",
        "    for i in indexes:\n",
        "        yield data[i]\n",
        "\n",
        "# And we'll want to take a gradient step for each data point. This approach\n",
        "#       leaves the possibility that we might circle around near a minimum forever,\n",
        "#       so whenever we stop getting improvement we'll decrease the step size and\n",
        "#       eventually quit:\n",
        "\n",
        "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "\n",
        "    data = zip(x, y)\n",
        "    theta = theta_0         # initial guess\n",
        "    alpha = alpha_0         # initial step size\n",
        "    min_theta, min_value = None, float(\"inf\")       # the minimum so far\n",
        "    iterations_with_no_improvement = 0\n",
        "\n",
        "    # if we ever go 100 iterations with no improvement, stop\n",
        "    while iterations_with_no_improvement < 100:\n",
        "        value = sum( target_fn(x_i, y_i, theta ) for x_i, y_i in data)\n",
        "\n",
        "        if value < min_value:\n",
        "            # if we've found a new minimum, remember it\n",
        "            # and go back to the original step size\n",
        "            min_theta, min_value = theta, value\n",
        "            iterations_with_no_improvement = 0\n",
        "        else:\n",
        "            # otherwise we're not impoving, so try shrinking the step size\n",
        "            iterations_with_no_improvement += 1\n",
        "            alpha *= 0.9\n",
        "\n",
        "        # and take a gradient step for each of the data points\n",
        "        for x_i, y_i in in_random_order(data):\n",
        "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
        "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
        "    \n",
        "    return min_theta\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "660GYiB9317J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The stochastic version will typically be alot faster than the batch version\n",
        "#       Of course, we'll want a version that maximizes as well:\n",
        "\n",
        "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "    return minimize_stochastic(negate(target_fn),\n",
        "                               negate_all(gradient_fn),\n",
        "                               x, y, theta_0, alpha_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs6I6t5z4XI8",
        "colab_type": "text"
      },
      "source": [
        "***For Further Explanation***\n",
        "\n",
        "\n",
        "\n",
        "*   Keep reading! We'll be using gradient descent to solve problems throughout the rest of the book\n",
        "*   At this point, you're undoubtedly sick of me recommending that you read textbooks. If it's any consolation, [Active Calculus](http://gvsu.edu/s/xr/) seems nicer that the calculus textbooks I learned from.\n",
        "\n",
        "* scikit-learn has a [Stochastic Gradient Descent module](https://scikit-learn.org/stable/modules/sgd.html) that is not as general as ours in some ways and more general in other ways. Really, though, in most real-world situations you'll be using libraries in which the optimization is already taken care of behind the scences, and you won't have to worry about it yourself ( other than when it doesn't work correctly, which one day, inevitably, it won't)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idM4GLwU4Uu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}